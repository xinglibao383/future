from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import os

# ========== å‚æ•°é…ç½® ==========
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "doc_demo"
EMBEDDING_MODEL = "BAAI/bge-large-zh"
EMBEDDING_DIM = 1024  # bge-large-zh ç»´åº¦

# ========== åˆå§‹åŒ–å‘é‡æ¨¡å‹ ==========
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)
model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())
    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)

def embed_text(text):
    text = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢: " + text
    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)
    with torch.no_grad():
        model_output = model(**encoded_input)
    embedding = mean_pooling(model_output, encoded_input['attention_mask'])
    return F.normalize(embedding, p=2, dim=1)[0].cpu().numpy()

# ========== è¿æ¥ Milvus ==========
connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)

# ========== åˆ›å»º Collection ==========
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=1000)
]
schema = CollectionSchema(fields, description="Demo collection")
collection = Collection(name=COLLECTION_NAME, schema=schema)
collection.create_index(field_name="embedding", index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 128}})
collection.load()

# ========== åŠ è½½æ–‡æ¡£å¹¶æ’å…¥ ==========
docs = [
    "æ³¨å†Œä¼šè®¡å¸ˆè€ƒè¯•åŒ…æ‹¬ã€Šä¼šè®¡ã€‹ã€Šå®¡è®¡ã€‹ã€Šç¨æ³•ã€‹ã€Šè´¢åŠ¡æˆæœ¬ç®¡ç†ã€‹ã€Šç»æµæ³•ã€‹ã€Šå…¬å¸æˆ˜ç•¥ä¸é£é™©ç®¡ç†ã€‹å…­é—¨ç§‘ç›®ã€‚",
    "è€ƒè¯•æ—¶é—´ä¸€èˆ¬å®‰æ’åœ¨æ¯å¹´çš„åæœˆã€‚",
    "è€ƒç”Ÿéœ€è¦é€šè¿‡æ‰€æœ‰ç§‘ç›®æ‰èƒ½è·å¾—å…¨ç§‘åˆæ ¼è¯ä¹¦ã€‚",
]

embeddings = [embed_text(d) for d in docs]
insert_data = [embeddings, docs]
collection.insert(insert_data)
collection.flush()

print("âœ… å‘é‡å·²å†™å…¥ Milvusï¼")

# ========== å‘é‡æŸ¥è¯¢ ==========
query_text = "æ³¨å†Œä¼šè®¡å¸ˆè€ƒä»€ä¹ˆå†…å®¹ï¼Ÿ"
query_vector = embed_text(query_text)

results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=3,
    output_fields=["text"]
)

print("\nğŸ” æŸ¥è¯¢ç»“æœï¼š")
for hit in results[0]:
    print(f"- ç›¸ä¼¼å†…å®¹: {hit.entity.get('text')}ï¼ˆè·ç¦»: {hit.distance:.4f}ï¼‰")