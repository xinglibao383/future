import warnings
warnings.filterwarnings("ignore")

import torchvision
torchvision.disable_beta_transforms_warning()

from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from sentence_transformers import SentenceTransformer
import torch
import requests
import json

# ========== å‚æ•°é…ç½® ==========
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "doc_demo"
EMBEDDING_DIM = 1024  # bge-large-zh æ¨¡å‹è¾“å‡ºç»´åº¦

OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen:7b"

# ========== åˆå§‹åŒ–å‘é‡æ¨¡å‹ ==========
device = "cuda" if torch.cuda.is_available() else "cpu"
embedding_model = SentenceTransformer("/home/xinglibao/workspace/future/embedding/bge-large-zh-v1.5")

def embed_text(text):
    prompt = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢: " + text
    embedding = embedding_model.encode(prompt, normalize_embeddings=True)
    return embedding

# ========== è¿æ¥ Milvus ==========
connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)

# ========== åˆ›å»ºæˆ–åŠ è½½ Collection ==========
if COLLECTION_NAME not in utility.list_collections():
    print(f"Collection {COLLECTION_NAME} ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º...")
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=1000)
    ]
    schema = CollectionSchema(fields, description="Demo collection")
    collection = Collection(name=COLLECTION_NAME, schema=schema)
    collection.create_index(field_name="embedding", index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 128}})
else:
    collection = Collection(name=COLLECTION_NAME)
collection.load()

# ========== æ’å…¥ç¤ºä¾‹æ–‡æ¡£ ==========
docs = [
    "æ³¨å†Œä¼šè®¡å¸ˆè€ƒè¯•åŒ…æ‹¬ã€Šä¼šè®¡ã€‹ã€Šå®¡è®¡ã€‹ã€Šç¨æ³•ã€‹ã€Šè´¢åŠ¡æˆæœ¬ç®¡ç†ã€‹ã€Šç»æµæ³•ã€‹ã€Šå…¬å¸æˆ˜ç•¥ä¸é£é™©ç®¡ç†ã€‹å…­é—¨ç§‘ç›®ã€‚",
    "è€ƒè¯•æ—¶é—´ä¸€èˆ¬å®‰æ’åœ¨æ¯å¹´çš„åæœˆã€‚",
    "è€ƒç”Ÿéœ€è¦é€šè¿‡æ‰€æœ‰ç§‘ç›®æ‰èƒ½è·å¾—å…¨ç§‘åˆæ ¼è¯ä¹¦ã€‚",
    "ä¸€èˆ¬ç¨‹åºå‘˜å–œæ¬¢åƒç«é”…ã€‚",
]

# Milvusæ’å…¥æ ¼å¼è¦æ±‚: æ¯ä¸ªå­—æ®µçš„åˆ—è¡¨æ•°æ®
embeddings = [embed_text(d) for d in docs]

# åˆ¤æ–­æ˜¯å¦å·²æ’å…¥è¿‡æ•°æ®ï¼Œé¿å…é‡å¤æ’å…¥
if collection.num_entities == 0:
    collection.insert([embeddings, docs])
    collection.flush()
    print("âœ… å‘é‡å·²å†™å…¥ Milvusï¼")
else:
    print("æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥ã€‚")

# ========== å®šä¹‰æ£€ç´¢å‡½æ•° ==========
def retrieve_documents(query, top_k=3):
    query_vector = embed_text(query)
    results = collection.search(
        data=[query_vector],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=top_k,
        output_fields=["text"]
    )
    retrieved_texts = [hit.entity.get("text") for hit in results[0]]
    return retrieved_texts

# ========== è°ƒç”¨ Ollama Qwen æ¨¡å‹ç”Ÿæˆå›ç­” ==========
def generate_answer(query, contexts):
    # æ‹¼æ¥ä¸Šä¸‹æ–‡ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€æ±‚è®¾è®¡æ‹¼æ¥æ ¼å¼ï¼‰
    context_text = "\n".join(contexts)
    prompt = f"åŸºäºä»¥ä¸‹å†…å®¹å°½å¯èƒ½è¯¦ç»†åœ°å›ç­”é—®é¢˜ï¼ŒåŒæ—¶è¿›è¡Œä¸€äº›æ‹“å±•ï¼š\n{context_text}\né—®é¢˜ï¼š{query}\nå›ç­”ï¼š"

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False
    }
    try:
        resp = requests.post(OLLAMA_API_URL, data=json.dumps(payload), headers={"Content-Type": "application/json"}, timeout=30)
        resp.raise_for_status()
        resp_json = resp.json()
        # Ollama è¿”å›çš„æ ¼å¼å¯èƒ½æ˜¯ { "results": [ { "text": "å›ç­”å†…å®¹" } ] }
        return resp_json.get("response", "").strip()
    except Exception as e:
        return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"

# ========== RAGæµç¨‹ ==========
def rag_pipeline(query):
    retrieved_docs = retrieve_documents(query)
    print("æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š")
    print(retrieved_docs)

    answer = generate_answer(query, retrieved_docs)
    return answer

# ========== ä¸»æµç¨‹ ==========
if __name__ == "__main__":
    user_query = "æ³¨å†Œä¼šè®¡å¸ˆè€ƒä»€ä¹ˆå†…å®¹ï¼Ÿ"
    final_answer = rag_pipeline(user_query)
    print("\nğŸ¤– ç”Ÿæˆçš„ç­”æ¡ˆï¼š")
    print(final_answer)